{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data__: Text samples belonging to various categories<br>\n",
    "__Data source:__ Collected by the team by scrapping text samples from various articles,blogs and websites. \n",
    "\n",
    "With the aim to classify future Queries based on its content, we used different machine learning algorithms can make more accurate predictions (i.e., classify the Query in one of the product categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "* [Goal](#obj)\n",
    "* [Importing packages and loading data](#imp)\n",
    "* [Exploratory Data Analysis (EDA) and Feature Engineering](#eda)\n",
    "* [Text Preprocessing](#pre)\n",
    "* [Multi-Classification models](#ml)\n",
    "    * [Spliting the data: train and test](#sp)\n",
    "    * [Models](#m)\n",
    "* [Comparison of model performance](#sum)\n",
    "* [Model Evaluation](#ev)\n",
    "    * [Precision, Recall, F1-score](#f1)\n",
    "    * [Confusion Matrix](#cm)\n",
    "* [Predictions](#pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='obj'></a>\n",
    "## Goal:<br>\n",
    "Classify Farmer queries into predefined categories.<br><br>\n",
    "Classification algorithms: Linear Support Vector Machine (LinearSVM), Random Forest, Multinomial Naive Bayes and Logistic Regression.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imp'></a>\n",
    "## Importing packages and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import seaborn as sns # used for plot interactive graph. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "df = pd.read_csv('bbc.csv',encoding='ANSI')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "## Exploratory Data Analysis (EDA) and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2).T # Columns are shown in rows for easy reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains features that are not necessary to solve our multi-classification problem. For this text classification problem, we are going to build another dataframe that contains ‘Query’ and ‘Type’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe with two columns\n",
    "df1 = df[['Type', 'Query']].copy()\n",
    "\n",
    "# Remove missing values (NaN)\n",
    "df1 = df1[pd.notnull(df1['Query'])]\n",
    "\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of complaints with text\n",
    "total = df1['Query'].notnull().sum()\n",
    "round((total/len(df)*100),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.Type.unique()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df1.Type.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br><br>Now we need to represent each class as a number, so as our predictive model can better understand the different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'category_id' with encoded categories \n",
    "df1['category_id'] = df1['Type'].factorize()[0]\n",
    "category_id_df = df1[['Type', 'category_id']].drop_duplicates()\n",
    "\n",
    "\n",
    "# Dictionaries for future use\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'Type']].values)\n",
    "\n",
    "# New dataframe\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to lowercase\n",
    "df1[\"Query\"]=df1[\"Query\"].str.lower()\n",
    "df1[\"Query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "query1 = []\n",
    "for line in df1[\"Query\"]:\n",
    "    result = re.sub(r'\\d+', '', line)\n",
    "    query1.append(result)\n",
    "df1[\"Query\"]=query1\n",
    "df1[\"Query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "query2 = []\n",
    "for line in df1[\"Query\"]:\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    line.translate(translator)\n",
    "    query2.append(line)\n",
    "df1[\"Query\"]=query2\n",
    "df1[\"Query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace from text\n",
    "query3 = []\n",
    "for line in df1[\"Query\"]:\n",
    "     \" \".join(line.split())\n",
    "     query3.append(line)\n",
    "df1[\"Query\"]=query3\n",
    "df1[\"Query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# remove stopwords function\n",
    "query_ = []\n",
    "for line in df1[\"Query\"]:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(line)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    query_.append(filtered_text)\n",
    "df1[\"Query\"]=query_\n",
    "df1[\"Query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert a list to string\n",
    "query_ = []\n",
    "for line in df1[\"Query\"]:\n",
    "    str1 = \"\"\n",
    "    for ele in line:\n",
    "      str1 += ele+\" \"\n",
    "    query_.append(str1)\n",
    "df1[\"Query\"]=query_\n",
    "df1[\"Query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# lemmatize string\n",
    "query5 = []\n",
    "for line in df1[\"Query\"]:\n",
    "    word_tokens = word_tokenize(line)\n",
    "    # provide context i.e. part-of-speech\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    query5.append(lemmas)\n",
    "df1[\"Query\"]=query5\n",
    "df1[\"Query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert a list to string\n",
    "query_ = []\n",
    "for line in df1[\"Query\"]:\n",
    "    str1 = \"\"\n",
    "    for ele in line:\n",
    "      str1 += ele+\" \"\n",
    "    query_.append(str1)\n",
    "df1[\"Query\"]=query_\n",
    "df1[\"Query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# stem words in the list of tokenised words\n",
    "query4 = []\n",
    "for line in df1[\"Query\"]:\n",
    "    word_tokens = word_tokenize(line)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    query4.append(stems)\n",
    "df1[\"Query\"]=query4\n",
    "df1[\"Query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert a list to string\n",
    "query_ = []\n",
    "for line in df1[\"Query\"]:\n",
    "    str1 = \"\"\n",
    "    for ele in line:\n",
    "      str1 += ele+\" \"\n",
    "    query_.append(str1)\n",
    "df1[\"Query\"]=query_\n",
    "df1[\"Query\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart below shows the number of text samples per category. It can be observed that The bar chart below shows the number of text samples per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "colors = ['grey','green','red','orange',\n",
    "    'darkblue','blue']\n",
    "df1.groupby('Type').Query.count().sort_values().plot.barh(\n",
    "    ylim=0, color=colors, title= 'NUMBER OF QUERY IN EACH CATEGORY\\n', fontsize = 12)\n",
    "plt.xlabel('Number of ocurrences', fontsize = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pre'></a>\n",
    "## Text Preprocessing\n",
    "\n",
    "The text needs to be transformed to vectors so as the algorithms will be able make predictions. In this case it will be used the Term Frequency – Inverse Document Frequency (TFIDF) weight to evaluate __how important a word is to a document in a collection of documents__.\n",
    "\n",
    "After removing __punctuation__ and __lower casing__ the words, importance of a word is determined in terms of its frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### “Term Frequency – Inverse Document Frequency \n",
    "\n",
    "__TF-IDF__ is the product of the __TF__ and __IDF__ scores of the term.<br><br> $$\\text{TF-IDF}=\\frac{\\text{TF}}{\\text{IDF}}$$<br>\n",
    "\n",
    "__Term Frequency :__ This summarizes how often a given word appears within a document.\n",
    "\n",
    "$$\\text{TF} = \\frac{\\text{Number of times the term appears in the doc}}{\\text{Total number of words in the doc}}$$<br><br>\n",
    "__Inverse Document Frequency:__ This downscales words that appear a lot across documents. A term has a high IDF score if it appears in a few documents. Conversely, if the term is very common among documents (i.e., “the”, “a”, “is”), the term would have a low IDF score.<br>\n",
    "\n",
    "$$\\text{IDF} = \\ln\\left(\\frac{\\text{Number of docs}}{\\text{Number docs the term appears in}} \\right)$$<br>\n",
    "\n",
    "TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents. The higher the TFIDF score, the rarer the term is. For instance, in a Weather query the word _rainfall_ would be mentioned fairly often. However, if we look at other queries, _rainfall_ probably would not show up in many of them. We can infer that _rainfall_ is most probably an important word in Weather queries as compared to the other categories. Therefore, _rainfall_ would have a high TF-IDF score for Weather queries.\n",
    "\n",
    "TfidfVectorizer class can be initialized with the following parameters:\n",
    "* __min_df__: remove the words from the vocabulary which have occurred in less than ‘min_df’ number of files.\n",
    "* __max_df__: remove the words from the vocabulary which have occurred in more than _‘max_df’ * total number of files in corpus_.\n",
    "* __sublinear_tf__: set to True to scale the term frequency in logarithmic scale.\n",
    "* __stop_words__: remove the predefined stop words in 'english'.\n",
    "* __use_idf__: weight factor must use inverse document frequency.\n",
    "* __ngram_range__: (1, 2) to indicate that unigrams and bigrams will be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(0, 2))\n",
    "\n",
    "# We transform each complaint into a vector\n",
    "features = tfidf.fit_transform(df1.Query).toarray()\n",
    "\n",
    "labels = df1.category_id\n",
    "\n",
    "print(\"Each of the %d Text samples is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the three most correlated terms with each of the product categories\n",
    "N = 3\n",
    "for Type, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2 = chi2(features, labels == category_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "  print(\"\\n==> %s:\" %(Type))\n",
    "  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n",
    "  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ml'></a>\n",
    "## Multi-Classification models\n",
    "\n",
    "The classification models evaluated are: \n",
    "* Random Forest\n",
    "* Linear Support Vector Machine\n",
    "* Multinomial Naive Bayes \n",
    "* Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sp'></a>\n",
    "### Spliting the data into train and test sets\n",
    "\n",
    "The original data was divided into features (X) and target (y), which were then splitted into train (75%) and test (25%) sets. Thus, the algorithms would be trained on one set of data and tested out on a completely different set of data (not seen before by the algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1['Query'] # Collection of documents\n",
    "y = df1['Type'] # Target or the labels we want to predict (i.e., the 13 different complaints of products)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m'></a>\n",
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "# 5 Cross-validation\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "    \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sum'></a>\n",
    "## Comparison of model performance\n",
    "\n",
    "The best mean acuracy was obtained with LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
    "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
    "\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, ignore_index=True)\n",
    "acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x='model_name', y='accuracy', \n",
    "            data=cv_df, \n",
    "            color='lightblue', \n",
    "            showmeans=True)\n",
    "plt.title(\"MEAN ACCURACY (cv = 5)\\n\", size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ev'></a>\n",
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features,labels, \n",
    "                                                                               df1.index, test_size=0.20, \n",
    "                                                                               random_state=1)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m'></a>\n",
    "### Precision, Recall, F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\n",
    "print(metrics.classification_report(y_test, y_pred,target_names= df1['Type'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to observe that the classes with more support (number of occurrences) tend to have a better f1-cscore. This is because the algorithm was trained with more data.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cm'></a>\n",
    "### Confusion Matrix\n",
    "\n",
    "A Confusion Matrix is a table which rows represent the actual class and columns represents the predicted class.<br><br>\n",
    "If we had a perfect model that always classifies correctly a new Query, then the confusion matrix would have values in the diagonal only (where predicted label = actual label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n",
    "            xticklabels=category_id_df.Type.values, \n",
    "            yticklabels=category_id_df.Type.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(\"CONFUSION MATRIX - LogisticRegression\\n\", size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the confusion matrix looks good (clear diagonal that represents correct classifications). Nevertheless, there are cases were the complaint was classified in a wrong class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most correlated terms with each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(features, labels)\n",
    "\n",
    "N = 4\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "  indices = np.argsort(model.coef_[category_id])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n",
    "  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n",
    "  print(\"\\n==> '{}':\".format(Product))\n",
    "  print(\"  * Top unigrams: %s\" %(', '.join(unigrams)))\n",
    "  print(\"  * Top bigrams: %s\" %(', '.join(bigrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pred'></a>\n",
    "## Predictions\n",
    "\n",
    "Now let's make a few predictions on unseen data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(0, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "fitted_vectorizer = tfidf.fit(X_train)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n",
    "\n",
    "model = LogisticRegression(random_state=0).fit(tfidf_vectorizer_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Query = \"\"\n",
    "print(model.predict(fitted_vectorizer.transform([new_Query])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features,labels, \n",
    "                                                                               df1.index, test_size=0.20, \n",
    "                                                                               random_state=1)\n",
    "model2 = LinearSVC()\n",
    "model2.fit(X_train, y_train)\n",
    "y_pred = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\n",
    "print(metrics.classification_report(y_test, y_pred,target_names= df1['Type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n",
    "            xticklabels=category_id_df.Type.values, \n",
    "            yticklabels=category_id_df.Type.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(\"CONFUSION MATRIX - LinearSVC\\n\", size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(0, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "fitted_vectorizer = tfidf.fit(X_train)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n",
    "\n",
    "model2 = LinearSVC().fit(tfidf_vectorizer_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Query2 = \"\"\n",
    "print(model2.predict(fitted_vectorizer.transform([new_Query2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features,labels, \n",
    "                                                                               df1.index, test_size=0.20, \n",
    "                                                                               random_state=1)\n",
    "model3 = MultinomialNB()\n",
    "model3.fit(X_train, y_train)\n",
    "y_pred = model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\n",
    "print(metrics.classification_report(y_test, y_pred,target_names= df1['Type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n",
    "            xticklabels=category_id_df.Type.values, \n",
    "            yticklabels=category_id_df.Type.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(\"CONFUSION MATRIX - MultinomialNB\\n\", size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(0, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "fitted_vectorizer = tfidf.fit(X_train)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n",
    "\n",
    "model3 = MultinomialNB().fit(tfidf_vectorizer_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Query3 = \"\"\n",
    "print(model3.predict(fitted_vectorizer.transform([new_Query3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features,labels, \n",
    "                                                                               df1.index, test_size=0.20, \n",
    "                                                                               random_state=1)\n",
    "model4 = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "model4.fit(X_train, y_train)\n",
    "y_pred = model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\n",
    "print(metrics.classification_report(y_test, y_pred,target_names= df1['Type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n",
    "            xticklabels=category_id_df.Type.values, \n",
    "            yticklabels=category_id_df.Type.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(\"CONFUSION MATRIX - RandomForest\\n\", size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(0, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "fitted_vectorizer = tfidf.fit(X_train)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n",
    "\n",
    "model4 = RandomForestClassifier().fit(tfidf_vectorizer_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Query4 = \"\"\n",
    "print(model4.predict(fitted_vectorizer.transform([new_Query4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1[df1['Query'] == new_Query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
